{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import fileinput\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import datetime\n",
    "import sys\n",
    "from tqdm  import tqdm\n",
    "tqdm.pandas()\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0600af701d2d03c2d3e7f952dbeac1cac9a6b13"
   },
   "outputs": [],
   "source": [
    "valid_forms = ['am','are','were','was','is','been','being','be']\n",
    "blank = '----'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "34b8a2b0038c21b865256563d376b0b5faaa7e4c"
   },
   "outputs": [],
   "source": [
    "def detect(tokens):\n",
    "    return [t for t in tokens if t in valid_forms]\n",
    "    \n",
    "def replace_blank(tokens):\n",
    "    return [blank if t in valid_forms else t for t in tokens]\n",
    "\n",
    "def create_windows(tokens, window_size=3):\n",
    "    X = []\n",
    "    for i, word in enumerate(tokens):\n",
    "        if word == blank:\n",
    "            window = tokens[i-window_size:i] + tokens[i+1:i+window_size+1]\n",
    "            window = ' '.join(window)\n",
    "            X.append(window)    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcf71adc4358539a3d158ee5f97daf4683d90365"
   },
   "outputs": [],
   "source": [
    "f1 = open(\"../input/internet_archive_scifi_v3.txt\",\"r\")\n",
    "a = f1.read()\n",
    "a = re.sub('[\\n]', '', a)\n",
    "tokens = wordpunct_tokenize(a)\n",
    "y = detect(tokens)\n",
    "tokens = replace_blank(tokens)\n",
    "X = create_windows(tokens)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eaa001a351805a031cfc44928333fab9304eae5b"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"Text\"] = X\n",
    "df[\"Label\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d39979b41d504a25bff4303e41aba5de29a9456"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba9e53d73830a70df82bda50ce6339ca8f7d205c"
   },
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(df[\"Label\"])\n",
    "df.drop(['Label'],axis=1,inplace=True)\n",
    "df = pd.concat([df,one_hot],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba3ca115ffafaf35f1eeedc1449b836ac1fa8a60"
   },
   "outputs": [],
   "source": [
    "df1 = df[:200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d92865168b510a57a218078f18c85d71883a903"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df1[\"Text\"].values, df1.drop(['Text'],axis=1).values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90b59bf2fe0b277e3e6bd4961e80fdb460132675"
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a66ac1534c360c1443d9b41e6377555e0d810fda"
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69419594e347fb579921b93b2657f00b585ad094"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 100, input_length=50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(8, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4321b53974828dde8364d4867978a309f77d4eab"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df58cce943e21007beb4a3fb9715691e38f26408"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "                    batch_size=1024,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ff81a21315b716a94d0d7d5966371dbc05526a1"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test,\n",
    "                       batch_size=256, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "658dc1479c287153b7f6a58487eeaf2692c88ada"
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4958e35b19fff351c2736be1c147b3cf3ba2b9c9"
   },
   "outputs": [],
   "source": [
    "print(classification_report(np.argmax(y_test,axis=1),np.argmax(preds,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f06e533fa0af9d8eccb9c0efb06813a9aff0e494"
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64f2c3a0c296d5bb724b405d18a8df438db444da"
   },
   "source": [
    "# BERT implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "02a6443556928bda3f2527b7d803361de3e8e518"
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b1a537730f8d6a59633a18e18443e422a3c7281"
   },
   "outputs": [],
   "source": [
    "import modeling\n",
    "import optimization\n",
    "import run_classifier\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1aa95f622d1b59645a9e22c2e6d5d2a5f29a2a50"
   },
   "outputs": [],
   "source": [
    "folder = 'model_folder'\n",
    "with zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20541f718c86fed1131409bcc9ee7736d3ec88ac"
   },
   "outputs": [],
   "source": [
    "BERT_MODEL = 'uncased_L-12_H-768_A-12'\n",
    "BERT_PRETRAINED_DIR = f'{folder}/uncased_L-12_H-768_A-12'\n",
    "OUTPUT_DIR = f'{folder}/outputs'\n",
    "print(f'>> Model output directory: {OUTPUT_DIR}')\n",
    "print(f'>>  BERT pretrained directory: {BERT_PRETRAINED_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "725094e6c9961e294f575672d4ee81449d745b1b"
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "df2[\"Text\"] = df[\"Text\"]\n",
    "df2[\"Label\"] = LabelEncoder().fit_transform(y)\n",
    "df2 = df2[:200000]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7990d9f6cdd2960ebfd61c40c58d3c1acb7d18b6"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df2[\"Text\"].values, df2[\"Label\"].values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a128c3f38ab69351e34f87977cec04d9d6c9189"
   },
   "outputs": [],
   "source": [
    "def create_examples(lines, set_type, labels=None):\n",
    "#Generate data for the BERT model\n",
    "    guid = f'{set_type}'\n",
    "    examples = []\n",
    "    if guid == 'train':\n",
    "        for line, label in zip(lines, labels):\n",
    "            text_a = line\n",
    "            label = str(label)\n",
    "            examples.append(\n",
    "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "    else:\n",
    "        for line in lines:\n",
    "            text_a = line\n",
    "            label = '0'\n",
    "            examples.append(\n",
    "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "    return examples\n",
    "\n",
    "# Model Hyper Parameters\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "WARMUP_PROPORTION = 0.1\n",
    "MAX_SEQ_LENGTH = 50\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 100000 #if you wish to finetune a model on a larger dataset, use larger interval\n",
    "# each checpoint weights about 1,5gb\n",
    "ITERATIONS_PER_LOOP = 100000\n",
    "NUM_TPU_CORES = 8\n",
    "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
    "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
    "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
    "\n",
    "label_list = [str(num) for num in range(8)]\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\n",
    "train_examples = create_examples(X_train, 'train', labels=y_train)\n",
    "\n",
    "tpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n",
    "#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
    "        num_shards=NUM_TPU_CORES,\n",
    "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
    "\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "model_fn = run_classifier.model_fn_builder(\n",
    "    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
    "    num_labels=len(label_list),\n",
    "    init_checkpoint=INIT_CHECKPOINT,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n",
    "    use_one_hot_embeddings=True)\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3cb95e4f486c7743add1039f996f501d07109fe"
   },
   "outputs": [],
   "source": [
    "print('Please wait...')\n",
    "train_features = run_classifier.convert_examples_to_features(\n",
    "    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "print('>> Started training at {} '.format(datetime.datetime.now()))\n",
    "print('  Num examples = {}'.format(len(train_examples)))\n",
    "print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
    "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "train_input_fn = run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=True)\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print('>> Finished training at {}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ed42f193d4586b10b58a65ea06c3d51385160bb"
   },
   "outputs": [],
   "source": [
    "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  all_input_ids = []\n",
    "  all_input_mask = []\n",
    "  all_segment_ids = []\n",
    "  all_label_ids = []\n",
    "\n",
    "  for feature in features:\n",
    "    all_input_ids.append(feature.input_ids)\n",
    "    all_input_mask.append(feature.input_mask)\n",
    "    all_segment_ids.append(feature.segment_ids)\n",
    "    all_label_ids.append(feature.label_id)\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    print(params)\n",
    "    batch_size = 500\n",
    "\n",
    "    num_examples = len(features)\n",
    "\n",
    "    d = tf.data.Dataset.from_tensor_slices({\n",
    "        \"input_ids\":\n",
    "            tf.constant(\n",
    "                all_input_ids, shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"input_mask\":\n",
    "            tf.constant(\n",
    "                all_input_mask,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"segment_ids\":\n",
    "            tf.constant(\n",
    "                all_segment_ids,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"label_ids\":\n",
    "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
    "    })\n",
    "\n",
    "    if is_training:\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "    return d\n",
    "\n",
    "  return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ddf519b690697635cbd3b47d2ee875feb94fd822"
   },
   "outputs": [],
   "source": [
    "predict_examples = create_examples(X_test, 'test')\n",
    "\n",
    "predict_features = run_classifier.convert_examples_to_features(\n",
    "    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "predict_input_fn = input_fn_builder(\n",
    "    features=predict_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "result = estimator.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb9635b3771ecc148b7dc7a2bf4f5de6a8173445"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for prediction in result:\n",
    "      preds.append(np.argmax(prediction['probabilities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f5cc9b9374f73ce94a04b9448e6807976cd89ec0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de003a2788e27d334232bd515572d73304192fe5"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy of BERT is:\",accuracy_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1586642280c06b07158651391f2e80b7756a7a3c"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d77113dca6782b19458f391d1d48769ad1e487a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
