{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numpy.random import choice\n",
    "\n",
    "# read data\n",
    "data = pd.read_feather(\"dataFrame.feather\")\n",
    "# encode categorical variable\n",
    "le = LabelEncoder()\n",
    "data[\"category\"] = le.fit_transform(data[\"category\"])\n",
    "# train-test split\n",
    "TRAIN_RATIO = 0.8\n",
    "train_ind = choice(data.shape[0], size = int(data.shape[0]*TRAIN_RATIO), replace = False)\n",
    "train_dt = data.iloc[train_ind,:]\n",
    "test_dt = data.drop(data.index[train_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"Desktop/output\"\n",
    "DATA_COLUMN = 'text'\n",
    "LABEL_COLUMN = 'category'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1, 2, 3, 4]device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = train_dt.loc[:,DATA_COLUMN]\n",
    "test_sentence = test_dt.loc[:,DATA_COLUMN]\n",
    "train_labels = train_dt.loc[:,LABEL_COLUMN]\n",
    "test_labels = test_dt.loc[:,LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "train_sentence = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in train_sentence]\n",
    "test_sentence = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in test_sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', '[CLS]', 'wine', 'comedy', 'wins', 'critics', \"'\", 'award', 'qui', '##rky', 'comedy', 'sideways', 'was', 'named', 'the', 'best', 'film', 'of', 'the', 'year', 'by', 'the', 'los', 'angeles', 'film', 'critics', 'association', '.', 'the', 'us', 'movie', 'also', 'picked', 'up', 'four', 'other', 'accolades', 'including', 'best', 'director', 'for', 'alexander', 'payne', 'and', 'supporting', 'actor', 'for', 'thomas', 'had', '##en', 'church', '.', 'british', 'actress', 'im', '##eld', '##a', 'st', '##au', '##nton', 'has', 'again', 'been', 'recognised', 'for', 'her', 'role', 'in', 'vera', 'drake', ',', 'winning', 'best', 'actress', ',', 'while', 'liam', 'nee', '##son', 'won', 'best', 'actor', 'for', 'kin', '##sey', '.', 'the', 'awards', 'will', 'be', 'handed', 'out', 'on', '13', 'january', 'at', 'a', 'ceremony', 'in', 'las', 'vegas', '.', 'sideways', 'tells', 'the', 'story', 'of', 'two', 'men', 'who', 'take', 'a', 'road', 'trip', 'through', 'california', \"'\", 's', 'wine', 'regions', 'and', 'also', 'stars', 'paul', 'gia', '##mat', '##ti', '.', 'virginia', 'mad', '##sen', 'was', 'also', 'named', 'best', 'supporting', 'actress', 'for', 'her', 'performance', 'in', 'the', 'film', '.', 'house', 'of', 'flying', 'daggers', ',', 'directed', 'by', 'yi', '##mo', '##u', 'zhang', ',', 'was', 'named', 'best', 'foreign', 'language', 'film', ',', 'while', 'the', 'animation', 'award', 'went', 'to', 'the', 'incredible', '##s', '.', 'unusually', ',', 'the', 'runners', '-', 'up', 'in', 'categories', 'were', 'also', 'named', 'with', 'clint', 'eastwood', \"'\", 's', 'million', 'dollar', 'baby', 'just', 'missing', 'out', 'on', 'the', 'best', 'film', 'award', '.', 'the', 'best', 'director', 'runner', '-', 'up', 'was', 'martin', 'sc', '##ors', '##ese', 'for', 'the', 'aviator', '.', 'a', 'career', 'achievement', 'award', 'will', 'be', 'handed', 'to', 'veteran', 'actor', 'and', 'comic', 'jerry', 'lewis', 'at', 'the', 'ceremony', 'next', 'year', '.', '[SEP]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_train_texts = [tokenizer.tokenize(sent) for sent in train_sentence]\n",
    "tokenized_test_texts = [tokenizer.tokenize(sent) for sent in test_sentence]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_train_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (628 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (703 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (971 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (801 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (800 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (813 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (607 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1055 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (798 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1025 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (832 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (953 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (683 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1109 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (731 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1022 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1004 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (760 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (697 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (809 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (946 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (682 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1027 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (664 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (707 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1188 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (994 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (760 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (887 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (880 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (928 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1225 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (784 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (780 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1334 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (704 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (569 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1071 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1087 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1219 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1062 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (738 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1221 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1113 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1089 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (806 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (893 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (900 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (778 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (795 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (846 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (671 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1027 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1059 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (931 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (870 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (604 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1114 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (627 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (733 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (680 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (734 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (957 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (645 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1079 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (799 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (967 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (677 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (654 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (957 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (698 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (824 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (741 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (693 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (752 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (625 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (682 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1088 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1059 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (781 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (795 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1059 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (994 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (683 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (972 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (598 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (645 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (905 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (5305 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1261 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (820 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (932 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (688 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (681 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (760 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (811 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (627 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1036 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (689 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (627 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (681 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2063 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (847 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (736 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (732 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (627 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (795 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (682 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (699 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (831 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (724 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (847 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (680 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (745 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (607 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (726 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (854 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (682 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (793 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (865 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1312 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (726 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (711 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (807 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (953 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (734 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (639 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1095 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (822 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (658 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (923 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (816 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (4349 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (722 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (805 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1121 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (767 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (965 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (703 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (869 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1138 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (921 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (821 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1013 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (911 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1095 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1027 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (617 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1154 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (764 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (607 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (944 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (702 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (810 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (779 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (888 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (693 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (760 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (825 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (878 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3005 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (848 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (828 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3645 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1071 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (820 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (802 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (771 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (771 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (704 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1109 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (693 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (681 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (754 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (713 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (683 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (921 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (627 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (706 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (749 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (628 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (715 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (644 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (628 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (900 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (810 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (808 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (739 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (787 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (625 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (959 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (979 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (706 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1132 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (834 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1148 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (684 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1058 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (675 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (750 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (752 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (812 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (807 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (809 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (979 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (783 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (689 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1136 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (851 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (811 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1565 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (885 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (814 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (851 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1170 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (787 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (972 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (789 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (915 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (665 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (748 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (905 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (897 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (919 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1105 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (628 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3112 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (852 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (871 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (813 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (771 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (736 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (861 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (702 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (751 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (901 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (914 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (753 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (645 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (796 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (757 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (847 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (845 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (718 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (699 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (830 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (910 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (666 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (890 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (767 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (732 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (688 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (775 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1077 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (727 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3886 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (767 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (743 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1396 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (803 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (604 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1242 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (955 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (868 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (681 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (753 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1641 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1635 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (887 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (724 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (785 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (569 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (779 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (701 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (671 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1028 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (808 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1284 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (837 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (809 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (888 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (789 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3072 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (644 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (976 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (741 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (665 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1048 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (747 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1087 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (725 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (819 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1010 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (705 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (953 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (882 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (627 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (664 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1669 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (884 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (962 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (845 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (699 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (791 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (696 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (801 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (789 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1091 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (659 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1000 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (926 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (725 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (617 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (966 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (790 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (790 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (831 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1225 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (736 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (736 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (705 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (760 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1085 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (869 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (930 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (872 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (663 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (737 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1017 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1025 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (795 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1111 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (627 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1285 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1094 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (876 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1997 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (735 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (724 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (701 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (711 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (663 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (724 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (763 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (666 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (720 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (705 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1399 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (682 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (876 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (796 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1032 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1014 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (906 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (804 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (942 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1672 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1033 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (654 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (995 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (733 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1758 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (915 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (903 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (776 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1050 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (737 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (700 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (760 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (968 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# index word encoding\n",
    "input_train_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_train_texts]\n",
    "input_test_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_test_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "MAX_LEN = 512\n",
    "input_train_ids = pad_sequences(input_train_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "input_test_ids = pad_sequences(input_test_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "train_attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_train_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  train_attention_masks.append(seq_mask)\n",
    "    \n",
    "    # Create attention masks\n",
    "test_attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_test_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  test_attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(input_train_ids)\n",
    "validation_inputs = torch.tensor(input_test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train_labels.values)\n",
    "validation_labels = torch.tensor(test_labels.values)\n",
    "\n",
    "train_masks = torch.tensor(train_attention_masks)\n",
    "validation_masks = torch.tensor(test_attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!! ZROZUM TEN FRAGMENT\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.optimization:t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/4 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 402653184 bytes. Error code 12 (Cannot allocate memory)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-005b538eed09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mtrain_loss_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/bert/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/bert/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/bert/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/bert/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/bert/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/bert/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/bert/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/bert/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/bert/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/bert/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, attention_mask)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mself_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/bert/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/bert/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 402653184 bytes. Error code 12 (Cannot allocate memory)\n"
     ]
    }
   ],
   "source": [
    "t = [] \n",
    "\n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  \n",
    "  \n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "  \n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    train_loss_set.append(loss.item())    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
